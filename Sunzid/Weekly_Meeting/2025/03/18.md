Vision branch: RGBD image > environment knowledge.
* Detect objects using YOLO.
* Get 3D coordinate of the detected objects.
* Update environment knowledge.

Olfaction branch: odor semantic and concentration.
* 2D odor dispersion model > turbulent concentration.
* E-nose chemical mix (VOC, CO, CO2, etc.) > lookup odor semantics.

Fusion * 2 behaviors: exploration and OSL
* Exploration: visit unseen parts of the environment to update environment knowledge.
* OSL: if odor concentration > threshold
	* Calculate cosine similarity of the detected odor type to the objects in environment knowledge. Generate a sorted list of object coordinates to visit.
	* Localize the top object â€“ if odor concentration decreases while moving towards it, localize the second object, and so on.
  * Source declaration: if the robot reaches within a threshold of the object, while odor detection keeps on rising.

* [ ] 1. Point-goal navigation while avoiding obstacles: SLAM or other techniques.
    	* We need something to scan the environment and to build a local map.
    	* See how other people use iThor simulator with navigation task.  
	
Validation:
* [ ] 2. Full house simulation (not just kitchen).
      * Check other's approach.  

Exploration:
* [ ] To-do 1: Exploration is rotating 4 times and getting the list of objects - coordinates.

Exploitation+Exploration:
* [ ] Use sentence transformer to check related objects.
* [ ] Move towards the top object.
* [ ] Exploration: While moving, keep updateing the environment knowledge (list of objects with coordinates).

* [ ] To-do 2: source probability map using olfaction reading.

* [ ] Update the AIRC paper.
