#### Semantic Vision and Olfaction-based Odor Source Localization
* **Goal**: design a navigation algorithm that uses semantic information of visual and olfactory information to deduce possible odor source location. 
* **Sample work**: Check this [paper](./../../../Reference/semantic_OSL.pdf) 

* **Related Works**:
  * Semantic Vision and Olfaction Fusion.
  * VirtualHome (or other in-door simulator) Env. and its applications with LLM.
  * Other multi-modal fusion algorithms (e.g., vision and audio fusion).#### Semantic Vision and Olfaction-based Odor Source Localization
* **Goal**: design a navigation algorithm that uses semantic information of visual and olfactory information to deduce possible odor source location. 
* **Sample work**: Check this [paper](./../../../Reference/semantic_OSL.pdf) 

* **Related Works**:
  * Semantic Vision and Olfaction Fusion.
  * VirtualHome (or other in-door simulator) Env. and its applications with LLM.
  * Other multi-modal fusion algorithms (e.g., vision and audio fusion).
  * **LX Added 02-07**: Object Goal Navigation (ObjectNav)
    * The ObjectNav is a task that requires a robot to navigate to find an object based on the provided object's name, e.g., 'bed'.
    * For household env., let olfaction suggest the object's name, let vision to find its location. 
    * During its visual search on the object, use olfaction to guide its direction (e.g., moving towards higher odor concentration areas)
    * Check this paper https://ieeexplore.ieee.org/document/10610712, we can also create a ``value map`` that involves both visual semantics and olfactory inference. 

* **Steps**:
  * Indoor Simulation Env. (household env. [VirtualHome](http://virtual-home.org/))
      * Mainstream indoor simulator by other - e.g., Virtual home.
      * Get a list of indoor simulator with different rooms (e.g., bathroom, living room...) - just info with project link here.
        * 2 main
          * [AI2Thor](https://ai2thor.allenai.org/) - citations: 972.
          * [Habitat-sim](https://github.com/facebookresearch/habitat-sim) - citations: 1530.
        * Others
          * [iGibson](https://stanfordvl.github.io/iGibson/intro.html) - citations: 218.
          * [Matterport3D](https://niessner.github.io/Matterport/) - citations: 362.
          * [Robots at Virtual Home](https://github.com/DavidFernandezChaves/RobotAtVirtualHome) - citations: 101.
          * [Minos](https://minosworld.github.io/) - citations: 284.
  * **Task Description**:
    * Simulate a search event: the robot is initiatialized at a random location within the home. The robot is tasked to find the odor source   location. The robot is controlled by LLM (local-based or GPT-4). The robot is asked to find the odor source location by analyzing both vision and olfaction signals. For vision signals provide a list of objects surrounding the robot. For olfaction signals provide two values - (1) the type of the odor and (2) the concentration of the odor. Currently, we provide some psudo odor information. To do that, we need a separate function to generate the odor distribution field. 
  
  * **Zero-shot LLM-based Navigation**
    * **Overview:**
      * Every time step, ask a LLM which room it is in (given the image captured from the front camera) 
      * -> I smell ``odor smell, e.g., smoke``, {a table of 3 actions, i.e., forward, turn left, and turn right}
      * Ask which action the robot should take.

      * [x] Teleport before the loop.
      * [x] Try separate system prompt with observation prompts.
      * [x] Olfaction data into the prompt
      * [ ] A top view map for plotting the trajectories.


| Action     | Obstacle | Objects |
|------------|---------|---------|
| Forward    |         |         |
| Turn Left  |         |         |
| Turn Right |         |         |



  
  
  * **Vision branch**: we need a vision branch to interpret surrounding objects to determine the room name (e.g., bathroom, living room, kitchen).
    * Comment: olfaction helps select the rooms, then vision is helping for selecting the specific odor source object.
    * After labeling rooms, narrow down the potential specific odor source object.
    * Use YOLO to extract object names -> use sentence transformer (or other ways) to connect the odor source object with the observed object.
    * [ ] We need to combine vision and olfaction.
      * [ ] Do this first: write a python control program to simulate the osl task. The program should be like:
        ```python
        While true:
           robot-initiatlization()   # randomly generate robot position
           target1 = vision-branch()   # query the camera to get the vision target
           target2 = olfaction-branch()   # query the olfaction sensor to get the olfaction target
           target = fusion(target1, target2)    # fuse the visual and olfactory targets to get a final target
           action = obstacle-avoidance(target)   # 
           robot-action(action)   # execute the command to move the robot position
           if find-the-source() or out-of-time():   # check whether or not to stop the loop
             break
        ```
      * Set a threshold to YOLO confidence to filter out unconfident objects.
      * The bounding box central point may not pointing to the object.
      * Follow the bounding box to reach the source.
      * Idea 2: don't use YOLO, provide image to vision language model (GPT-4, BLIP), ask which action should be performed to avoid obstacles and to approach the odor source.
      * [ ] Use sentence transformers (or other way) to choose top 3 odor source candidates.
      * [ ] When the robot moves toward the top candidate, verify this decision using olfaction. In other words, the concentration should increase when the robot moves toward the true odor source.
      * [ ] If the top candidate doesn't provide high odor concentration, go back to number 2, and so on.
      * [ ] We need to know the object coordinates.
      * [ ] Make this process autonumous, i.e., upon random initialization, the robot should be able to localize the odor source automatically.

   
  * Olfaction branch:

  * Real-world Env. with multiple odor source locations. 




* **Possible publications:**
  * Review/survey paper regarding multi-sensory fusion in robotics

  * **LX Added 02-07**: Object Goal Navigation (ObjectNav)
    * The ObjectNav is a task that requires a robot to navigate to find an object based on the provided object's name, e.g., 'bed'.
    * For household env., let olfaction suggest the object's name, let vision to find its location. 
    * During its visual search on the object, use olfaction to guide its direction (e.g., moving towards higher odor concentration areas)
    * Check this paper https://ieeexplore.ieee.org/document/10610712, we can also create a ``value map`` that involves both visual semantics and olfactory inference. 

* **Steps**:
  * Indoor Simulation Env. (household env. [VirtualHome](http://virtual-home.org/))
      * Mainstream indoor simulator by other - e.g., Virtual home.
      * Get a list of indoor simulator with different rooms (e.g., bathroom, living room...) - just info with project link here.
        * 2 main
          * [AI2Thor](https://ai2thor.allenai.org/) - citations: 972.
          * [Habitat-sim](https://github.com/facebookresearch/habitat-sim) - citations: 1530.
        * Others
          * [iGibson](https://stanfordvl.github.io/iGibson/intro.html) - citations: 218.
          * [Matterport3D](https://niessner.github.io/Matterport/) - citations: 362.
          * [Robots at Virtual Home](https://github.com/DavidFernandezChaves/RobotAtVirtualHome) - citations: 101.
          * [Minos](https://minosworld.github.io/) - citations: 284.
  * **Task Description**:
    * Simulate a search event: the robot is initiatialized at a random location within the home. The robot is tasked to find the odor source   location. The robot is controlled by LLM (local-based or GPT-4). The robot is asked to find the odor source location by analyzing both vision and olfaction signals. For vision signals provide a list of objects surrounding the robot. For olfaction signals provide two values - (1) the type of the odor and (2) the concentration of the odor. Currently, we provide some psudo odor information. To do that, we need a separate function to generate the odor distribution field. 
  
  * **Zero-shot LLM-based Navigation**
    * **Overview:**
      * Every time step, ask a LLM which room it is in (given the image captured from the front camera) 
      * -> I smell ``odor smell, e.g., smoke``, {a table of 3 actions, i.e., forward, turn left, and turn right}
      * Ask which action the robot should take.

      * [x] Teleport before the loop.
      * [x] Try separate system prompt with observation prompts.
      * [x] A top view map for plotting the trajectories.
      * [ ] Plot:
        * [ ] Add arrow of robot movement direction.
      * [ ] Olfaction data needs to be corrected.
        * [ ] We also need a plot of plume superpositioned on the trajectory plot.


| Action     | Obstacle | Objects |
|------------|---------|---------|
| Forward    |         |         |
| Turn Left  |         |         |
| Turn Right |         |         |



  
  
  * **Vision branch**: we need a vision branch to interpret surrounding objects to determine the room name (e.g., bathroom, living room, kitchen).
    * Comment: olfaction helps select the rooms, then vision is helping for selecting the specific odor source object.
    * After labeling rooms, narrow down the potential specific odor source object.
    * Use YOLO to extract object names -> use sentence transformer (or other ways) to connect the odor source object with the observed object.
    * [ ] We need to combine vision and olfaction.
      * [ ] Do this first: write a python control program to simulate the osl task. The program should be like:
        ```python
        While true:
           robot-initiatlization()   # randomly generate robot position
           target1 = vision-branch()   # query the camera to get the vision target
           target2 = olfaction-branch()   # query the olfaction sensor to get the olfaction target
           target = fusion(target1, target2)    # fuse the visual and olfactory targets to get a final target
           action = obstacle-avoidance(target)   # 
           robot-action(action)   # execute the command to move the robot position
           if find-the-source() or out-of-time():   # check whether or not to stop the loop
             break
        ```
      * Set a threshold to YOLO confidence to filter out unconfident objects.
      * The bounding box central point may not pointing to the object.
      * Follow the bounding box to reach the source.
      * Idea 2: don't use YOLO, provide image to vision language model (GPT-4, BLIP), ask which action should be performed to avoid obstacles and to approach the odor source.
      * [ ] Use sentence transformers (or other way) to choose top 3 odor source candidates.
      * [ ] When the robot moves toward the top candidate, verify this decision using olfaction. In other words, the concentration should increase when the robot moves toward the true odor source.
      * [ ] If the top candidate doesn't provide high odor concentration, go back to number 2, and so on.
      * [ ] We need to know the object coordinates.
      * [ ] Make this process autonumous, i.e., upon random initialization, the robot should be able to localize the odor source automatically.

   
  * Olfaction branch:

  * Real-world Env. with multiple odor source locations. 




* **Possible publications:**
  * Review/survey paper regarding multi-sensory fusion in robotics
