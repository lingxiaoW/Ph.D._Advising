#### Semantic Vision and Olfaction-based Odor Source Localization
* **Goal**: design a navigation algorithm that uses semantic information of visual and olfactory information to deduce possible odor source location. 
* **Sample work**: Check this [paper](./../../../Reference/semantic_OSL.pdf) 

* **Related Works**:
  * Semantic Vision and Olfaction Fusion.
  * VirtualHome (or other in-door simulator) Env. and its applications with LLM.
  * Other multi-modal fusion algorithms (e.g., vision and audio fusion).

* **Steps**:
  * Indoor Simulation Env. (household env. [VirtualHome](http://virtual-home.org/))
      * Mainstream indoor simulator by other - e.g., Virtual home.
      * Get a list of indoor simulator with different rooms (e.g., bathroom, living room...) - just info with project link here.
        * 2 main
          * [AI2Thor](https://ai2thor.allenai.org/) - citations: 972.
          * [Habitat-sim](https://github.com/facebookresearch/habitat-sim) - citations: 1530.
        * Others
          * [iGibson](https://stanfordvl.github.io/iGibson/intro.html) - citations: 218.
          * [Matterport3D](https://niessner.github.io/Matterport/) - citations: 362.
          * [Robots at Virtual Home](https://github.com/DavidFernandezChaves/RobotAtVirtualHome) - citations: 101.
          * [Minos](https://minosworld.github.io/) - citations: 284.
          
  * Vision branch: we need a vision branch to interpret surrounding objects to determine the room name (e.g., bathroom, living room, kitchen).
    * [x] Install the AI2Thor simulation environment.
    * [x] Run a robotic agent in the simulation. Get familier with how to control the robotic agent in the simulation environment.
    * [x] How to incorporate customized navigation agent with the simulated robot.
      * [x] Write an agnet control code, let the agent loop inside the simulator (forward 10 sec, small left 10 sec, ...)
      * [x] Write code to get a list of object names and positions within the current view.
        * Comment: we can get a list of ground truth object names within the view.
        * Comment 2: we can use YOLO to detect objects within the view.
    * [x] Run the clip model to label rooms inside the simuation environment.
    * Comment: olfaction helps select the rooms, then vision is helping for selecting the specific odor source object.
    * After labeling rooms, narrow down the potential specific odor source object.
    * [ ] We need a python-based control program which can process the current view to extract object names.
      * [ ] Simulate a search event: the robot is initiatialized at a random location within the home. The robot is tasked to find the odor source location. The robot is controlled by LLM (local-based or GPT-4). The robot is asked to find the odor source location by analyzing both vision and olfaction signals. For vision signals provide a list of objects surrounding the robot. For olfaction signals provide two values - (1) the type of the odor and (2) the concentration of the odor. Currently, we provide some psudo odor information. To do that, we need a separate function to generate the odor distribution field.
      * [ ] Choose an object as the reasonable odor source, for instance a bowl on the table, the water sink, the oven, etc.
      * [ ] Based on the odor source object choice, give a reasonable name for the odor type, for instance, bad apple smell-bowl on the table, gas smell-oven, etc.
   
  * Olfaction branch:
      * [ ] We need a separate python code to simulate odor distribution. Check the reference paper to find Gaussian distribution equation. The input of the python code is the robot position, and the output of the python code is the odor concentration.

  * Real-world Env. with multiple odor source locations. 
* **Possible publications:**
  * Review/survey paper regarding multi-sensory fusion in robotics



#### A Paper Reading Tool
* [Zotero](https://www.zotero.org/)
* How to use Zotero?
  * Tutorial 1: https://libguides.unm.edu/zotero
  * Tutorial 2: https://www.zotero.org/support/quick_start_guide
