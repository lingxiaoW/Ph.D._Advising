### CycleGAN
* DQN verification
  * Train a DQN with generated_real_images
  * Train another DQN with simulated_images
  * Compare these two DQNs in real-world plume data (PIV image)
    * Averaged search time, success rate, travel distance.
    * You can try flip real-world plume data. 
    * Expectation: the DQN trained with generated_real_images should be better than Simulated_images
    * Another try: DDPG - Deep Deterministic Policy Gradient
    * Updated in 09-20: Summarize your current result. 
      * Collect robot trajectory files, images, and the table 
      * Save them into a folder
    * Update in 09-20: DDPG output [-1 1] 
      * What are actions? dist in x and y
      * output -> [0.5 0.4]
      * change the output into 1 variable [-1 1] -> heading command
      * -1 -> -90 degree; 1 -> 90
      * if the output is 0 -> 0 degree
      * heading command = current heading + DDPG output
      * newx = oldx + cos(heading command); newy = oldy + sin(heading command)
    

* Paper writing ideas
  * Sim-to-real gap
  * In OSL problem, sim-to-real gap is in plume data -> our solution is CycleGAN
  * How to approve CycleGAN is valid? Use DQN
 
* [x] Desktop setup
  * Connect the desktop (with GPU) with a monitor, mouse, keyboard.
  * Ask IT to setup a remote control in your desktop.
  * Use the desktop (with GPU) for training. 


### LX Updated
* Your current contract ends up at **June 30, 2025**, under the support of a one-year project. If I don't bring new research money before **June 30, 2025**, I don't have the money to support you anymore. 
  
* Bringing research money is not one-man job. You need to work as hard as I am. You cannot enjoy your day while I spin like a spinning top. You need to complete **the assigned research tasks fast**, so that I can write proposals to bring new research money. 

* Learning is self-motivated. Nobody will pay you to learn. 
  
